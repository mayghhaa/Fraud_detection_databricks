{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f935e3-6f67-4c14-9778-6f085a765bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bffcd66a-fb9f-4dd5-8694-5a3319901426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data modeling is the process of creating a data model for the data to be stored in a database. This process involves defining and structuring the data elements and their relationships to one another. In the context of Databricks, data modeling is crucial for setting up new tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5b16b4b-16c7-4582-861a-f7b470a6d239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- USERS TABLE\n",
    "CREATE TABLE IF NOT EXISTS users (\n",
    "    user_id STRING NOT NULL,\n",
    "    default_city STRING,\n",
    "    default_state STRING,\n",
    "    CONSTRAINT pk_users PRIMARY KEY (user_id)\n",
    ") USING DELTA;\n",
    "\n",
    "-- DEVICES TABLE\n",
    "CREATE TABLE IF NOT EXISTS devices (\n",
    "    device_id STRING NOT NULL,\n",
    "    device_os STRING,\n",
    "    CONSTRAINT pk_devices PRIMARY KEY (device_id)\n",
    ") USING DELTA;\n",
    "\n",
    "-- RECIPIENTS TABLE (Merchants)\n",
    "CREATE TABLE IF NOT EXISTS recipients (\n",
    "    merchant_id STRING NOT NULL,\n",
    "    merchant_category STRING,\n",
    "    CONSTRAINT pk_recipients PRIMARY KEY (merchant_id)\n",
    ") USING DELTA;\n",
    "\n",
    "-- TRANSACTIONS TABLE\n",
    "CREATE TABLE IF NOT EXISTS transactions (\n",
    "    transaction_id STRING NOT NULL,\n",
    "    transaction_date DATE,\n",
    "    transaction_time STRING,\n",
    "    user_id STRING NOT NULL,\n",
    "    merchant_id STRING NOT NULL,\n",
    "    device_id STRING NOT NULL,\n",
    "    upi_channel STRING,\n",
    "    transaction_city STRING,\n",
    "    transaction_state STRING,\n",
    "    ip_address STRING,\n",
    "    transaction_status STRING,\n",
    "    amount FLOAT,\n",
    "    transaction_amount_deviation FLOAT,\n",
    "    fraud INT,\n",
    "    CONSTRAINT pk_transactions PRIMARY KEY (transaction_id),\n",
    "    CONSTRAINT fk_txn_user FOREIGN KEY (user_id) REFERENCES users(user_id),\n",
    "    CONSTRAINT fk_txn_device FOREIGN KEY (device_id) REFERENCES devices(device_id),\n",
    "    CONSTRAINT fk_txn_merchant FOREIGN KEY (merchant_id) REFERENCES recipients(merchant_id)\n",
    ") USING DELTA;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d034913-5270-4f14-bbf3-51b06c2543c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Script for Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2aa89d9-869b-4a22-8d1c-8a602cf512f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, FloatType, DateType, IntegerType\n",
    ")\n",
    "from pyspark.sql.functions import col, hour, unix_timestamp, when, date_format\n",
    "\n",
    "# For demonstration, using a static list:\n",
    "city_state_dict = {\n",
    "    \"Mumbai\": \"Maharashtra\",\n",
    "    \"Delhi\": \"Delhi\",\n",
    "    \"Bangalore\": \"Karnataka\",\n",
    "    \"Hyderabad\": \"Telangana\",\n",
    "    \"Ahmedabad\": \"Gujarat\",\n",
    "    \"Chennai\": \"Tamil Nadu\",\n",
    "    \"Kolkata\": \"West Bengal\",\n",
    "    \"Pune\": \"Maharashtra\",\n",
    "    \"Jaipur\": \"Rajasthan\",\n",
    "    \"Surat\": \"Gujarat\",\n",
    "    \"Lucknow\": \"Uttar Pradesh\",\n",
    "    \"Kanpur\": \"Uttar Pradesh\",\n",
    "    \"Nagpur\": \"Maharashtra\",\n",
    "    \"Indore\": \"Madhya Pradesh\",\n",
    "    \"Thane\": \"Maharashtra\",\n",
    "    \"Bhopal\": \"Madhya Pradesh\",\n",
    "    \"Visakhapatnam\": \"Andhra Pradesh\",\n",
    "    \"Pimpri-Chinchwad\": \"Maharashtra\",\n",
    "    \"Patna\": \"Bihar\",\n",
    "    \"Vadodara\": \"Gujarat\",\n",
    "    \"Ghaziabad\": \"Uttar Pradesh\",\n",
    "    \"Ludhiana\": \"Punjab\",\n",
    "    \"Agra\": \"Uttar Pradesh\",\n",
    "    \"Nashik\": \"Maharashtra\",\n",
    "    \"Faridabad\": \"Haryana\",\n",
    "    \"Meerut\": \"Uttar Pradesh\",\n",
    "    \"Rajkot\": \"Gujarat\",\n",
    "    \"Kalyan-Dombivli\": \"Maharashtra\",\n",
    "    \"Vasai-Virar\": \"Maharashtra\",\n",
    "    \"Varanasi\": \"Uttar Pradesh\",\n",
    "    \"Srinagar\": \"Jammu and Kashmir\",\n",
    "    \"Aurangabad\": \"Maharashtra\",\n",
    "    \"Dhanbad\": \"Jharkhand\",\n",
    "    \"Amritsar\": \"Punjab\",\n",
    "    \"Navi Mumbai\": \"Maharashtra\",\n",
    "    \"Prayagraj\": \"Uttar Pradesh\",\n",
    "    \"Howrah\": \"West Bengal\",\n",
    "    \"Ranchi\": \"Jharkhand\",\n",
    "    \"Jabalpur\": \"Madhya Pradesh\",\n",
    "    \"Gwalior\": \"Madhya Pradesh\",\n",
    "    \"Coimbatore\": \"Tamil Nadu\",\n",
    "    \"Vijayawada\": \"Andhra Pradesh\",\n",
    "    \"Jodhpur\": \"Rajasthan\",\n",
    "    \"Madurai\": \"Tamil Nadu\",\n",
    "    \"Raipur\": \"Chhattisgarh\",\n",
    "    \"Kota\": \"Rajasthan\",\n",
    "    \"Guwahati\": \"Assam\",\n",
    "    \"Chandigarh\": \"Chandigarh\",\n",
    "    \"Solapur\": \"Maharashtra\",\n",
    "    \"Hubli-Dharwad\": \"Karnataka\",\n",
    "    \"Tiruchirappalli\": \"Tamil Nadu\",\n",
    "    \"Bareilly\": \"Uttar Pradesh\",\n",
    "    \"Mysore\": \"Karnataka\",\n",
    "    \"Tiruppur\": \"Tamil Nadu\",\n",
    "    \"Gurgaon\": \"Haryana\",\n",
    "    \"Aligarh\": \"Uttar Pradesh\",\n",
    "    \"Jalandhar\": \"Punjab\",\n",
    "    \"Bhubaneswar\": \"Odisha\",\n",
    "    \"Salem\": \"Tamil Nadu\",\n",
    "    \"Mira-Bhayandar\": \"Maharashtra\",\n",
    "    \"Warangal\": \"Telangana\",\n",
    "    \"Thiruvananthapuram\": \"Kerala\",\n",
    "    \"Guntur\": \"Andhra Pradesh\",\n",
    "    \"Bhiwandi\": \"Maharashtra\",\n",
    "    \"Saharanpur\": \"Uttar Pradesh\",\n",
    "    \"Gorakhpur\": \"Uttar Pradesh\",\n",
    "    \"Bikaner\": \"Rajasthan\",\n",
    "    \"Amravati\": \"Maharashtra\",\n",
    "    \"Noida\": \"Uttar Pradesh\",\n",
    "    \"Jamshedpur\": \"Jharkhand\",\n",
    "    \"Bhilai\": \"Chhattisgarh\",\n",
    "    \"Cuttack\": \"Odisha\",\n",
    "    \"Firozabad\": \"Uttar Pradesh\",\n",
    "    \"Kochi\": \"Kerala\",\n",
    "    \"Nellore\": \"Andhra Pradesh\",\n",
    "    \"Bhavnagar\": \"Gujarat\",\n",
    "    \"Dehradun\": \"Uttarakhand\",\n",
    "    \"Durgapur\": \"West Bengal\",\n",
    "    \"Asansol\": \"West Bengal\",\n",
    "    \"Nanded\": \"Maharashtra\",\n",
    "    \"Kolhapur\": \"Maharashtra\",\n",
    "    \"Ajmer\": \"Rajasthan\",\n",
    "    \"Gulbarga\": \"Karnataka\",\n",
    "    \"Jamnagar\": \"Gujarat\",\n",
    "    \"Ujjain\": \"Madhya Pradesh\",\n",
    "    \"Loni\": \"Uttar Pradesh\",\n",
    "    \"Siliguri\": \"West Bengal\",\n",
    "    \"Jhansi\": \"Uttar Pradesh\",\n",
    "    \"Ulhasnagar\": \"Maharashtra\",\n",
    "    \"Jammu\": \"Jammu and Kashmir\",\n",
    "    \"Sangli-Miraj & Kupwad\": \"Maharashtra\",\n",
    "    \"Mangalore\": \"Karnataka\",\n",
    "    \"Erode\": \"Tamil Nadu\",\n",
    "    \"Belgaum\": \"Karnataka\",\n",
    "    \"Ambattur\": \"Tamil Nadu\",\n",
    "    \"Tirunelveli\": \"Tamil Nadu\",\n",
    "    \"Malegaon\": \"Maharashtra\",\n",
    "    \"Gaya\": \"Bihar\",\n",
    "    \"Jalgaon\": \"Maharashtra\",\n",
    "    \"Udaipur\": \"Rajasthan\",\n",
    "    \"Maheshtala\": \"West Bengal\",\n",
    "    \"Davanagere\": \"Karnataka\",\n",
    "    \"Kozhikode\": \"Kerala\",\n",
    "    \"Kurnool\": \"Andhra Pradesh\",\n",
    "    \"Akola\": \"Maharashtra\",\n",
    "    \"Rajpur Sonarpur\": \"West Bengal\",\n",
    "    \"Rajahmundry\": \"Andhra Pradesh\",\n",
    "    \"Bokaro\": \"Jharkhand\",\n",
    "    \"South Dumdum\": \"West Bengal\",\n",
    "    \"Bellary\": \"Karnataka\",\n",
    "    \"Patiala\": \"Punjab\",\n",
    "    \"Gopalpur\": \"Odisha\",\n",
    "    \"Agartala\": \"Tripura\",\n",
    "    \"Bhagalpur\": \"Bihar\",\n",
    "    \"Muzaffarnagar\": \"Uttar Pradesh\",\n",
    "    \"Bhatpara\": \"West Bengal\",\n",
    "    \"Panihati\": \"West Bengal\",\n",
    "    \"Latur\": \"Maharashtra\",\n",
    "    \"Dhule\": \"Maharashtra\",\n",
    "    \"Tirupati\": \"Andhra Pradesh\",\n",
    "    \"Rohtak\": \"Haryana\",\n",
    "    \"Korba\": \"Chhattisgarh\",\n",
    "    \"Bhilwara\": \"Rajasthan\",\n",
    "    \"Berhampur\": \"Odisha\",\n",
    "    \"Muzaffarpur\": \"Bihar\",\n",
    "    \"Ahmednagar\": \"Maharashtra\",\n",
    "    \"Mathura\": \"Uttar Pradesh\",\n",
    "    \"Kollam\": \"Kerala\",\n",
    "    \"Avadi\": \"Tamil Nadu\",\n",
    "    \"Kadapa\": \"Andhra Pradesh\",\n",
    "    \"Kamarhati\": \"West Bengal\",\n",
    "    \"Bilaspur\": \"Chhattisgarh\",\n",
    "    \"Shahjahanpur\": \"Uttar Pradesh\",\n",
    "    \"Bijapur\": \"Karnataka\",\n",
    "    \"Rampur\": \"Uttar Pradesh\",\n",
    "    \"Shivamogga\": \"Karnataka\",\n",
    "    \"Chandrapur\": \"Maharashtra\",\n",
    "    \"Junagadh\": \"Gujarat\",\n",
    "    \"Thrissur\": \"Kerala\",\n",
    "    \"Alwar\": \"Rajasthan\",\n",
    "    \"Bardhaman\": \"West Bengal\",\n",
    "    \"Kulti\": \"West Bengal\",\n",
    "    \"Kakinada\": \"Andhra Pradesh\",\n",
    "    \"Nizamabad\": \"Telangana\",\n",
    "    \"Parbhani\": \"Maharashtra\",\n",
    "    \"Tumkur\": \"Karnataka\",\n",
    "    \"Khammam\": \"Telangana\",\n",
    "    \"Ozhukarai\": \"Puducherry\",\n",
    "    \"Bihar Sharif\": \"Bihar\",\n",
    "    \"Panipat\": \"Haryana\",\n",
    "    \"Darbhanga\": \"Bihar\",\n",
    "    \"Bally\": \"West Bengal\",\n",
    "    \"Aizawl\": \"Mizoram\",\n",
    "    \"Dewas\": \"Madhya Pradesh\",\n",
    "    \"Ichalkaranji\": \"Maharashtra\",\n",
    "    \"Karnal\": \"Haryana\",\n",
    "    \"Bathinda\": \"Punjab\",\n",
    "    \"Jalna\": \"Maharashtra\",\n",
    "    \"Eluru\": \"Andhra Pradesh\",\n",
    "    \"Kirari Suleman Nagar\": \"Delhi\",\n",
    "    \"Barasat\": \"West Bengal\",\n",
    "    \"Purnia\": \"Bihar\",\n",
    "    \"Satna\": \"Madhya Pradesh\",\n",
    "    \"Mau\": \"Uttar Pradesh\",\n",
    "    \"Sonipat\": \"Haryana\",\n",
    "    \"Farrukhabad\": \"Uttar Pradesh\",\n",
    "    \"Sagar\": \"Madhya Pradesh\",\n",
    "    \"Rourkela\": \"Odisha\",\n",
    "    \"Durg\": \"Chhattisgarh\",\n",
    "    \"Imphal\": \"Manipur\",\n",
    "    \"Ratlam\": \"Madhya Pradesh\",\n",
    "    \"Hapur\": \"Uttar Pradesh\",\n",
    "    \"Arrah\": \"Bihar\",\n",
    "    \"Karimnagar\": \"Telangana\",\n",
    "    \"Anantapur\": \"Andhra Pradesh\",\n",
    "    \"Etawah\": \"Uttar Pradesh\",\n",
    "    \"Ambarnath\": \"Maharashtra\",\n",
    "    \"North Dumdum\": \"West Bengal\",\n",
    "    \"Bharatpur\": \"Rajasthan\",\n",
    "    \"Begusarai\": \"Bihar\",\n",
    "    \"New Delhi\": \"Delhi\",\n",
    "    \"Gandhidham\": \"Gujarat\",\n",
    "    \"Baranagar\": \"West Bengal\",\n",
    "    \"Tiruvottiyur\": \"Tamil Nadu\",\n",
    "    \"Puducherry\": \"Puducherry\",\n",
    "    \"Sikar\": \"Rajasthan\",\n",
    "    \"Thoothukudi\": \"Tamil Nadu\",\n",
    "    \"Rewa\": \"Madhya Pradesh\",\n",
    "    \"Mirzapur\": \"Uttar Pradesh\",\n",
    "    \"Raichur\": \"Karnataka\",\n",
    "    \"Pali\": \"Rajasthan\"\n",
    "}\n",
    "# (Paste the above dictionary here)\n",
    "\n",
    "cities = list(city_state_dict.keys())\n",
    "states = list(set(city_state_dict.values()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fake = Faker('en_IN')\n",
    "random.seed(42)\n",
    "Faker.seed(42)\n",
    "\n",
    "NUM_USERS = 2000\n",
    "NUM_DEVICES = 1000\n",
    "NUM_RECIPIENTS = 1500\n",
    "NUM_TRANSACTIONS = 20000\n",
    "\n",
    "\n",
    "\n",
    "transaction_statuses = [\"Completed\", \"Pending\", \"Failed\"]\n",
    "\n",
    "def random_date(start, end):\n",
    "    return start + timedelta(days=random.randint(0, (end - start).days))\n",
    "\n",
    "# USERS TABLE\n",
    "# Prepare the list of cities from the city_state_dict\n",
    "cities = list(city_state_dict.keys())\n",
    "\n",
    "user_rows, user_ids, user_city_map = [], [], {}\n",
    "for i in range(1, NUM_USERS + 1):\n",
    "    user_id = str(i)\n",
    "    default_city = random.choice(cities)\n",
    "    default_state = city_state_dict[default_city]\n",
    "\n",
    "    user_rows.append((user_id, default_city, default_state))\n",
    "    user_ids.append(user_id)\n",
    "    # Sample 3 known cities for the user from the same city list\n",
    "    user_city_map[user_id] = set(random.sample(cities, min(3, len(cities))))\n",
    "user_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), False),\n",
    "    StructField(\"default_city\", StringType(), True),\n",
    "    StructField(\"default_state\", StringType(), True)\n",
    "])\n",
    "users_df = spark.createDataFrame(user_rows, schema=user_schema)\n",
    "users_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"upi_fraud_schema.users\")\n",
    "\n",
    "# DEVICES TABLE\n",
    "device_rows, device_ids, user_to_devices = [], [], defaultdict(list)\n",
    "for i in range(1, NUM_DEVICES + 1):\n",
    "    device_id = str(i)\n",
    "    os_types = ['Android', 'iOS', 'Windows', 'Other']\n",
    "    weights = [60.7, 25.0, 11.18, 1.12]\n",
    "    norm_weights = [w / sum(weights) for w in weights]\n",
    "    device_os = random.choices(os_types, weights=norm_weights)[0]\n",
    "\n",
    "    user_id = random.choice(user_ids)\n",
    "    device_rows.append((device_id, device_os))\n",
    "    device_ids.append(device_id)\n",
    "    user_to_devices[user_id].append(device_id)\n",
    "device_schema = StructType([\n",
    "    StructField(\"device_id\", StringType(), False),\n",
    "    StructField(\"device_os\", StringType(), True)\n",
    "])\n",
    "devices_df = spark.createDataFrame(device_rows, schema=device_schema)\n",
    "devices_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"upi_fraud_schema.devices\")\n",
    "\n",
    "# RECIPIENTS TABLE (Merchants)\n",
    "recipient_rows, merchant_ids = [], []\n",
    "for i in range(1, NUM_RECIPIENTS + 1):\n",
    "    merchant_id = str(i)\n",
    "\n",
    "    # Proper distribution for merchant categories\n",
    "    merchant_categories = ['Utilities', 'Shopping', 'Food', 'Travel', 'Education', 'Healthcare']\n",
    "    weights = [35, 25, 15, 10, 8, 7]\n",
    "    norm_weights = [w / sum(weights) for w in weights]\n",
    "    merchant_category = random.choices(merchant_categories, weights=norm_weights)[0]\n",
    "\n",
    "    recipient_rows.append((merchant_id, merchant_category))\n",
    "    merchant_ids.append(merchant_id)\n",
    "recipient_schema = StructType([\n",
    "    StructField(\"merchant_id\", StringType(), False),\n",
    "    StructField(\"merchant_category\", StringType(), True)\n",
    "])\n",
    "recipients_df = spark.createDataFrame(recipient_rows, schema=recipient_schema)\n",
    "recipients_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"upi_fraud_schema.recipients\")\n",
    "\n",
    "# TRANSACTIONS TABLE\n",
    "transaction_rows = []\n",
    "for i in range(1, NUM_TRANSACTIONS + 1):\n",
    "    transaction_id = str(i)\n",
    "    # Probabilities constraint on days \n",
    "    # Step 1: Define probabilities\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "    weights = [20, 10, 9, 11,18 , 19, 13 ]\n",
    "    norm_weights = [w / sum(weights) for w in weights]\n",
    "\n",
    "    # Step 2: Pick a day of week\n",
    "    chosen_day = random.choices(days, weights=norm_weights)[0]\n",
    "\n",
    "    # Step 3: Find all dates in range that match chosen day\n",
    "    start_date = datetime(2023, 1, 1)\n",
    "    end_date = datetime(2024, 6, 30)\n",
    "    matching_dates = []\n",
    "    current = start_date\n",
    "    while current <= end_date:\n",
    "        if current.strftime('%A') == chosen_day:\n",
    "            matching_dates.append(current)\n",
    "        current += timedelta(days=1)\n",
    "\n",
    "    # Step 4: Pick a random date from those\n",
    "    date_obj = random.choice(matching_dates)\n",
    "    transaction_date = date_obj.date()\n",
    "\n",
    "    # Step 5: Generate random time\n",
    "    transaction_time = fake.time(pattern=\"%I:%M:%S %p\")\n",
    "\n",
    "    user_id = random.choice(user_ids)\n",
    "    merchant_id = random.choice(merchant_ids)\n",
    "    possible_devices = user_to_devices[user_id]\n",
    "    device_id = random.choice(possible_devices) if possible_devices else random.choice(device_ids)\n",
    "    # Randomise the UPI channels data  \n",
    "    upi_channels = [\"PhonePe\", \"GPay\", \"PayTM\", \"BHIM\"]\n",
    "    weights_upi = [0.52, 0.25, 0.15, 0.08]  # These represent the probabilities\n",
    "    upi_channel = random.choices(upi_channels, weights=weights_upi)[0]\n",
    "\n",
    "    transaction_city = random.choice(list(city_state_dict.keys()))\n",
    "    transaction_state = city_state_dict[transaction_city]\n",
    "    ip_address = fake.ipv4()\n",
    "    transaction_status = random.choice(transaction_statuses)\n",
    "    \n",
    "    def get_realistic_amount(category):\n",
    "        r = random.random()\n",
    "        if category == \"Education\" or category == \"Healthcare\":\n",
    "            if r < 0.75:\n",
    "                return round(random.uniform(1000, 50000), 2)\n",
    "            elif r < 0.95:\n",
    "                return round(random.uniform(50001, 200000), 2)\n",
    "            else:\n",
    "                return round(random.uniform(200001, 500000), 2)  # Rare, high-value\n",
    "        elif category == \"Travel\":\n",
    "            if r < 0.85:\n",
    "                return round(random.uniform(500, 20000), 2)\n",
    "            else:\n",
    "                return round(random.uniform(20001, 100000), 2)\n",
    "        elif category == \"Shopping\":\n",
    "            if r < 0.9:\n",
    "                return round(random.uniform(100, 10000), 2)\n",
    "            else:\n",
    "                return round(random.uniform(10001, 100000), 2)\n",
    "        elif category == \"Utilities\":\n",
    "            if r < 0.95:\n",
    "                return round(random.uniform(50, 2000), 2)\n",
    "            else:\n",
    "                return round(random.uniform(2001, 10000), 2)\n",
    "        elif category == \"Food\":\n",
    "            if r < 0.98:\n",
    "                return round(random.uniform(50, 2000), 2)\n",
    "            else:\n",
    "                return round(random.uniform(2001, 10000), 2)\n",
    "        else:\n",
    "            return round(random.uniform(1, 5000), 2)\n",
    "    \n",
    "    amount = get_realistic_amount(merchant_category)\n",
    "    transaction_amount_deviation = round(random.uniform(-100, 100), 2)\n",
    "    # --- Improved Risk Score Calculation ---\n",
    "    risk_score = 0\n",
    "\n",
    "    # Category-aware high amount thresholds (add this dict at the top of your script)\n",
    "    category_thresholds = {\n",
    "        \"Utilities\": 2000,\n",
    "        \"Shopping\": 10000,\n",
    "        \"Food\": 2000,\n",
    "        \"Travel\": 20000,\n",
    "        \"Education\": 200000,\n",
    "        \"Healthcare\": 200000\n",
    "    }\n",
    "    high_amount_threshold = category_thresholds.get(merchant_category, 5000)\n",
    "\n",
    "    # High amount buckets (now category-aware)\n",
    "    if amount > high_amount_threshold:\n",
    "        risk_score += 40\n",
    "    elif amount > high_amount_threshold * 0.6:\n",
    "        risk_score += 25\n",
    "    elif amount > high_amount_threshold * 0.2:\n",
    "        risk_score += 10\n",
    "\n",
    "    # Large deviation from user's normal transaction amount (relative if possible)\n",
    "    if 'user_avg_amount' in locals() and user_avg_amount > 0:\n",
    "        deviation_pct = abs(amount - user_avg_amount) / user_avg_amount\n",
    "        if deviation_pct > 1.0:\n",
    "            risk_score += 25\n",
    "        elif deviation_pct > 0.5:\n",
    "            risk_score += 10\n",
    "    else:\n",
    "        if abs(transaction_amount_deviation) > 100:\n",
    "            risk_score += 25\n",
    "        elif abs(transaction_amount_deviation) > 50:\n",
    "            risk_score += 10\n",
    "\n",
    "    # Location anomaly: city or state not in user's known set\n",
    "    if transaction_city not in user_city_map[user_id]:\n",
    "        risk_score += 15\n",
    "    if transaction_state not in [city_state_dict[city] for city in user_city_map[user_id]]:\n",
    "        risk_score += 10\n",
    "\n",
    "    # Device anomaly: device rarely used by this user\n",
    "    if device_id not in user_to_devices[user_id][:2]:  # not among user's 2 most common devices\n",
    "        risk_score += 10\n",
    "\n",
    "    # Odd hour: between 12 AM and 5 AM (fix for 12 AM edge case)\n",
    "    from datetime import datetime\n",
    "    txn_hour = datetime.strptime(transaction_time, \"%I:%M:%S %p\").hour\n",
    "    if 0 <= txn_hour < 5:\n",
    "        risk_score += 15\n",
    "\n",
    "    # Status and amount interaction\n",
    "    if transaction_status == 'Failed' and amount > 3000:\n",
    "        risk_score += 20\n",
    "    elif transaction_status == 'Pending' and amount > 3000:\n",
    "        risk_score += 10\n",
    "\n",
    "\n",
    "    # Normalize and assign fraud label\n",
    "    risk_score = min(risk_score / 100, 1.0)\n",
    "    fraud = 1 if risk_score > 0.7 else 0\n",
    "\n",
    "\n",
    "    transaction_rows.append((\n",
    "        transaction_id, transaction_date, transaction_time, user_id, merchant_id, device_id, upi_channel,\n",
    "        transaction_city, transaction_state, ip_address, transaction_status, amount, transaction_amount_deviation, fraud\n",
    "    ))\n",
    "\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), False),\n",
    "    StructField(\"transaction_date\", DateType(), True),\n",
    "    StructField(\"transaction_time\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), False),\n",
    "    StructField(\"merchant_id\", StringType(), False),\n",
    "    StructField(\"device_id\", StringType(), False),\n",
    "    StructField(\"upi_channel\", StringType(), True),\n",
    "    StructField(\"transaction_city\", StringType(), True),\n",
    "    StructField(\"transaction_state\", StringType(), True),\n",
    "    StructField(\"ip_address\", StringType(), True),\n",
    "    StructField(\"transaction_status\", StringType(), True),\n",
    "    StructField(\"amount\", FloatType(), True),\n",
    "    StructField(\"transaction_amount_deviation\", FloatType(), True),\n",
    "    StructField(\"fraud\", IntegerType(), True)\n",
    "])\n",
    "transactions_df = spark.createDataFrame(transaction_rows, schema=transaction_schema)\n",
    "transactions_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"upi_fraud_schema.transactions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c873231-4027-4e9f-9ce8-43e81765aae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Adding joins on tables to create \"Features\" Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3566a1ae-12e9-436e-9def-d5e2cf9c371d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour, unix_timestamp, when, date_format, col\n",
    "\n",
    "# Read tables\n",
    "users_df = spark.table(\"upi_fraud_schema.users\")\n",
    "devices_df = spark.table(\"upi_fraud_schema.devices\")\n",
    "recipients_df = spark.table(\"upi_fraud_schema.recipients\")\n",
    "transactions_df = spark.table(\"upi_fraud_schema.transactions\")\n",
    "\n",
    "# Join and engineer features\n",
    "features_df = transactions_df.alias(\"t\") \\\n",
    "    .join(users_df.alias(\"u\"), col(\"t.user_id\") == col(\"u.user_id\")) \\\n",
    "    .join(devices_df.alias(\"d\"), col(\"t.device_id\") == col(\"d.device_id\")) \\\n",
    "    .join(recipients_df.alias(\"r\"), col(\"t.merchant_id\") == col(\"r.merchant_id\")) \\\n",
    "    .withColumn(\"hour\", hour(unix_timestamp(col(\"t.transaction_time\"), \"hh:mm:ss a\").cast(\"timestamp\"))) \\\n",
    "    .withColumn(\"day_of_week\", date_format(col(\"t.transaction_date\"), \"EEEE\")) \\\n",
    "    .withColumn(\"is_high_value\", (col(\"t.amount\") > 3000).cast(\"int\")) \\\n",
    "    .withColumn(\"is_odd_hour\", when((col(\"hour\") < 6) | (col(\"hour\") > 22), 1).otherwise(0)) \\\n",
    "    .select(\n",
    "        \"t.transaction_id\", \"t.user_id\", \"t.merchant_id\", \"t.device_id\",\n",
    "        \"t.amount\", \"hour\", \"day_of_week\",\n",
    "        \"t.transaction_amount_deviation\", \"is_high_value\", \"is_odd_hour\",\n",
    "        \"t.upi_channel\", \"t.transaction_status\",\n",
    "        \"r.merchant_category\", \"d.device_os\", \"u.default_city\", \"u.default_state\",\n",
    "        \"t.transaction_city\", \"t.transaction_state\", \"t.fraud\",\n",
    "        \"t.transaction_date\", \"t.transaction_time\"\n",
    "    )\n",
    "\n",
    "features_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"upi_fraud_schema.features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c90482-9bf7-4838-98d9-e7515032a577",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752055264150}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>transaction_id</th><th>user_id</th><th>merchant_id</th><th>device_id</th><th>amount</th><th>hour</th><th>day_of_week</th><th>transaction_amount_deviation</th><th>is_high_value</th><th>is_odd_hour</th><th>upi_channel</th><th>transaction_status</th><th>merchant_category</th><th>device_os</th><th>default_city</th><th>default_state</th><th>transaction_city</th><th>transaction_state</th><th>fraud</th><th>transaction_date</th><th>transaction_time</th></tr></thead><tbody><tr><td>1</td><td>1614</td><td>889</td><td>389</td><td>293.01</td><td>18</td><td>Saturday</td><td>-78.0</td><td>0</td><td>0</td><td>PhonePe</td><td>Completed</td><td>Food</td><td>Android</td><td>Jaipur</td><td>Rajasthan</td><td>Ichalkaranji</td><td>Maharashtra</td><td>0</td><td>2023-10-14</td><td>06:40:37 PM</td></tr><tr><td>2</td><td>675</td><td>1312</td><td>855</td><td>1606.29</td><td>18</td><td>Friday</td><td>-45.27</td><td>0</td><td>0</td><td>PhonePe</td><td>Pending</td><td>Utilities</td><td>iOS</td><td>Ujjain</td><td>Madhya Pradesh</td><td>Loni</td><td>Uttar Pradesh</td><td>0</td><td>2023-03-03</td><td>06:46:35 PM</td></tr><tr><td>3</td><td>776</td><td>216</td><td>802</td><td>647.77</td><td>0</td><td>Monday</td><td>64.27</td><td>0</td><td>1</td><td>BHIM</td><td>Failed</td><td>Utilities</td><td>Android</td><td>Rajahmundry</td><td>Andhra Pradesh</td><td>Raipur</td><td>Chhattisgarh</td><td>0</td><td>2023-07-17</td><td>12:43:03 AM</td></tr><tr><td>4</td><td>715</td><td>1435</td><td>586</td><td>1798.31</td><td>21</td><td>Thursday</td><td>-65.85</td><td>0</td><td>0</td><td>PayTM</td><td>Pending</td><td>Utilities</td><td>Windows</td><td>Mysore</td><td>Karnataka</td><td>Kulti</td><td>West Bengal</td><td>0</td><td>2024-04-18</td><td>09:59:19 PM</td></tr><tr><td>5</td><td>371</td><td>707</td><td>826</td><td>1706.78</td><td>2</td><td>Sunday</td><td>-4.63</td><td>0</td><td>1</td><td>GPay</td><td>Failed</td><td>Utilities</td><td>Android</td><td>Alwar</td><td>Rajasthan</td><td>Chandrapur</td><td>Maharashtra</td><td>1</td><td>2023-12-10</td><td>02:22:35 AM</td></tr><tr><td>6</td><td>1407</td><td>1052</td><td>788</td><td>571.37</td><td>11</td><td>Monday</td><td>83.69</td><td>0</td><td>0</td><td>GPay</td><td>Pending</td><td>Utilities</td><td>iOS</td><td>Etawah</td><td>Uttar Pradesh</td><td>Howrah</td><td>West Bengal</td><td>0</td><td>2023-01-09</td><td>11:51:40 AM</td></tr><tr><td>7</td><td>1449</td><td>511</td><td>345</td><td>8387.95</td><td>16</td><td>Friday</td><td>0.45</td><td>1</td><td>0</td><td>GPay</td><td>Pending</td><td>Shopping</td><td>Android</td><td>Thiruvananthapuram</td><td>Kerala</td><td>Guwahati</td><td>Assam</td><td>1</td><td>2023-12-01</td><td>04:10:50 PM</td></tr><tr><td>8</td><td>1067</td><td>1472</td><td>903</td><td>219.54</td><td>23</td><td>Tuesday</td><td>-38.87</td><td>0</td><td>1</td><td>BHIM</td><td>Pending</td><td>Utilities</td><td>Android</td><td>Jalna</td><td>Maharashtra</td><td>Salem</td><td>Tamil Nadu</td><td>0</td><td>2024-01-16</td><td>11:22:49 PM</td></tr><tr><td>9</td><td>1300</td><td>163</td><td>79</td><td>214.94</td><td>13</td><td>Saturday</td><td>-42.83</td><td>0</td><td>0</td><td>GPay</td><td>Completed</td><td>Travel</td><td>Other</td><td>Jabalpur</td><td>Madhya Pradesh</td><td>Nashik</td><td>Maharashtra</td><td>0</td><td>2023-03-25</td><td>01:56:43 PM</td></tr><tr><td>10</td><td>1971</td><td>869</td><td>639</td><td>9104.76</td><td>19</td><td>Sunday</td><td>39.58</td><td>1</td><td>0</td><td>PhonePe</td><td>Failed</td><td>Shopping</td><td>Android</td><td>Gurgaon</td><td>Haryana</td><td>Bilaspur</td><td>Chhattisgarh</td><td>1</td><td>2023-01-08</td><td>07:34:53 PM</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1",
         "1614",
         "889",
         "389",
         293.01,
         18,
         "Saturday",
         -78.0,
         0,
         0,
         "PhonePe",
         "Completed",
         "Food",
         "Android",
         "Jaipur",
         "Rajasthan",
         "Ichalkaranji",
         "Maharashtra",
         0,
         "2023-10-14",
         "06:40:37 PM"
        ],
        [
         "2",
         "675",
         "1312",
         "855",
         1606.29,
         18,
         "Friday",
         -45.27,
         0,
         0,
         "PhonePe",
         "Pending",
         "Utilities",
         "iOS",
         "Ujjain",
         "Madhya Pradesh",
         "Loni",
         "Uttar Pradesh",
         0,
         "2023-03-03",
         "06:46:35 PM"
        ],
        [
         "3",
         "776",
         "216",
         "802",
         647.77,
         0,
         "Monday",
         64.27,
         0,
         1,
         "BHIM",
         "Failed",
         "Utilities",
         "Android",
         "Rajahmundry",
         "Andhra Pradesh",
         "Raipur",
         "Chhattisgarh",
         0,
         "2023-07-17",
         "12:43:03 AM"
        ],
        [
         "4",
         "715",
         "1435",
         "586",
         1798.31,
         21,
         "Thursday",
         -65.85,
         0,
         0,
         "PayTM",
         "Pending",
         "Utilities",
         "Windows",
         "Mysore",
         "Karnataka",
         "Kulti",
         "West Bengal",
         0,
         "2024-04-18",
         "09:59:19 PM"
        ],
        [
         "5",
         "371",
         "707",
         "826",
         1706.78,
         2,
         "Sunday",
         -4.63,
         0,
         1,
         "GPay",
         "Failed",
         "Utilities",
         "Android",
         "Alwar",
         "Rajasthan",
         "Chandrapur",
         "Maharashtra",
         1,
         "2023-12-10",
         "02:22:35 AM"
        ],
        [
         "6",
         "1407",
         "1052",
         "788",
         571.37,
         11,
         "Monday",
         83.69,
         0,
         0,
         "GPay",
         "Pending",
         "Utilities",
         "iOS",
         "Etawah",
         "Uttar Pradesh",
         "Howrah",
         "West Bengal",
         0,
         "2023-01-09",
         "11:51:40 AM"
        ],
        [
         "7",
         "1449",
         "511",
         "345",
         8387.95,
         16,
         "Friday",
         0.45,
         1,
         0,
         "GPay",
         "Pending",
         "Shopping",
         "Android",
         "Thiruvananthapuram",
         "Kerala",
         "Guwahati",
         "Assam",
         1,
         "2023-12-01",
         "04:10:50 PM"
        ],
        [
         "8",
         "1067",
         "1472",
         "903",
         219.54,
         23,
         "Tuesday",
         -38.87,
         0,
         1,
         "BHIM",
         "Pending",
         "Utilities",
         "Android",
         "Jalna",
         "Maharashtra",
         "Salem",
         "Tamil Nadu",
         0,
         "2024-01-16",
         "11:22:49 PM"
        ],
        [
         "9",
         "1300",
         "163",
         "79",
         214.94,
         13,
         "Saturday",
         -42.83,
         0,
         0,
         "GPay",
         "Completed",
         "Travel",
         "Other",
         "Jabalpur",
         "Madhya Pradesh",
         "Nashik",
         "Maharashtra",
         0,
         "2023-03-25",
         "01:56:43 PM"
        ],
        [
         "10",
         "1971",
         "869",
         "639",
         9104.76,
         19,
         "Sunday",
         39.58,
         1,
         0,
         "PhonePe",
         "Failed",
         "Shopping",
         "Android",
         "Gurgaon",
         "Haryana",
         "Bilaspur",
         "Chhattisgarh",
         1,
         "2023-01-08",
         "07:34:53 PM"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "transaction_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "user_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "merchant_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "device_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "amount",
            "nullable": true,
            "type": "float"
           },
           {
            "metadata": {},
            "name": "hour",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "day_of_week",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "transaction_amount_deviation",
            "nullable": true,
            "type": "float"
           },
           {
            "metadata": {},
            "name": "is_high_value",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "is_odd_hour",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "upi_channel",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "transaction_status",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "merchant_category",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "device_os",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "default_city",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "default_state",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "transaction_city",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "transaction_state",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "fraud",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "transaction_date",
            "nullable": true,
            "type": "date"
           },
           {
            "metadata": {},
            "name": "transaction_time",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 1
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "transaction_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "merchant_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "device_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "hour",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "day_of_week",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "transaction_amount_deviation",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "is_high_value",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "is_odd_hour",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "upi_channel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "transaction_status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "merchant_category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "device_os",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "default_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "default_state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "transaction_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "transaction_state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "fraud",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "transaction_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "transaction_time",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM upi_fraud_schema.features LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05c17c9d-0f3b-41f7-a624-99a9bb426044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Saving df as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f0b28b-2aa2-434a-8f37-cea2865980c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as 'upi_transactions_2024.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=features_df\n",
    "pdf=features_df.toPandas()\n",
    "\n",
    "\n",
    "pdf.to_csv('upi_transactions.csv', index=False)\n",
    "print(\"Dataset saved as 'upi_transactions_2024.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8b8d9f0-48ce-4e53-ad80-3e4a78efc196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######IMPORTING LIBRARIES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e016127-fc88-48b5-8652-f8b654b57668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "# %pip install pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as px\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "815d8956-79bb-4bd5-8f32-0fee77468a46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "View Schema of the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cebd7834-4f7e-41bc-8a77-ab103ab8a90a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['transaction_id',\n",
       " 'user_id',\n",
       " 'merchant_id',\n",
       " 'device_id',\n",
       " 'amount',\n",
       " 'hour',\n",
       " 'day_of_week',\n",
       " 'transaction_amount_deviation',\n",
       " 'is_high_value',\n",
       " 'is_odd_hour',\n",
       " 'upi_channel',\n",
       " 'transaction_status',\n",
       " 'merchant_category',\n",
       " 'device_os',\n",
       " 'default_city',\n",
       " 'default_state',\n",
       " 'transaction_city',\n",
       " 'transaction_state',\n",
       " 'fraud',\n",
       " 'transaction_date',\n",
       " 'transaction_time']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "# df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4837d21-f366-4cf3-b96a-f612c47d8a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Preparation\n",
    "### -duplicate, missing, unique, removing or dropping unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "213e426e-8523-4ab1-8b2c-9c80cc6ed4e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data preparation is a crucial step in the data analysis and machine learning pipeline. It involves cleaning, transforming, and organizing raw data into a format suitable for analysis. Key tasks in data preparation include:\n",
    "\n",
    "- Cleaning Data: Removing duplicates, handling missing values, and correcting errors to ensure data quality.\n",
    "- Transforming Data: Standardizing data types, normalizing values, and creating new features to enhance the dataset.\n",
    "- Organizing Data: Structuring data into tables or schemas that facilitate efficient querying and analysis.\n",
    "\n",
    "Effective data preparation improves the accuracy and reliability of the analysis and models built on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b33a9b9-8b85-4e6f-b988-b22243268b4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### DUPLICATE VALUES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "589c510f-1c1f-4f1a-b88f-35ab0dc26366",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+-----------+---------+------+----+-----------+----------------------------+-------------+-----------+-----------+------------------+-----------------+---------+------------+-------------+----------------+-----------------+-----+----------------+----------------+-----+\n|transaction_id|user_id|merchant_id|device_id|amount|hour|day_of_week|transaction_amount_deviation|is_high_value|is_odd_hour|upi_channel|transaction_status|merchant_category|device_os|default_city|default_state|transaction_city|transaction_state|fraud|transaction_date|transaction_time|count|\n+--------------+-------+-----------+---------+------+----+-----------+----------------------------+-------------+-----------+-----------+------------------+-----------------+---------+------------+-------------+----------------+-----------------+-----+----------------+----------------+-----+\n+--------------+-------+-----------+---------+------+----+-----------+----------------------------+-------------+-----------+-----------+------------------+-----------------+---------+------------+-------------+----------------+-----------------+-----+----------------+----------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Find all rows that are duplicated across all columns\n",
    "df.groupBy(df.columns) \\\n",
    "    .count() \\\n",
    "    .filter(\"count > 1\") \\\n",
    "    .show(truncate=False)\n",
    "    #  .count()\n",
    "\n",
    "\n",
    "# df.duplictaed()->pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "009f706c-bdd6-4797-bb5a-217452b4f2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c22ae1-3dc2-4826-bc1b-84ec8cee8ae1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+-----------+---------+------+----+-----------+----------------------------+-------------+-----------+-----------+------------------+-----------------+---------+------------+-------------+----------------+-----------------+-----+----------------+----------------+\n|transaction_id|user_id|merchant_id|device_id|amount|hour|day_of_week|transaction_amount_deviation|is_high_value|is_odd_hour|upi_channel|transaction_status|merchant_category|device_os|default_city|default_state|transaction_city|transaction_state|fraud|transaction_date|transaction_time|\n+--------------+-------+-----------+---------+------+----+-----------+----------------------------+-------------+-----------+-----------+------------------+-----------------+---------+------------+-------------+----------------+-----------------+-----+----------------+----------------+\n|             0|      0|          0|        0|     0|   0|          0|                           0|            0|          0|          0|                 0|                0|        0|           0|            0|               0|                0|    0|               0|               0|\n+--------------+-------+-----------+---------+------+----+-----------+----------------------------+-------------+-----------+-----------+------------------+-----------------+---------+------------+-------------+----------------+-----------------+-----+----------------+----------------+\n\n+--------------+-------+-----------+---------+------+----+-----------+----------------------------+-------------+-----------+-----------+------------------+-----------------+---------+------------+-------------+----------------+-----------------+-----+----------------+----------------+\n|transaction_id|user_id|merchant_id|device_id|amount|hour|day_of_week|transaction_amount_deviation|is_high_value|is_odd_hour|upi_channel|transaction_status|merchant_category|device_os|default_city|default_state|transaction_city|transaction_state|fraud|transaction_date|transaction_time|\n+--------------+-------+-----------+---------+------+----+-----------+----------------------------+-------------+-----------+-----------+------------------+-----------------+---------+------------+-------------+----------------+-----------------+-----+----------------+----------------+\n+--------------+-------+-----------+---------+------+----+-----------+----------------------------+-------------+-----------+-----------+------------------+-----------------+---------+------------+-------------+----------------+-----------------+-----+----------------+----------------+\n\nRows with any missing value: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "from functools import reduce\n",
    "\n",
    "# This will show you how many nulls are present in each column:\n",
    "missing_counts = df.select([\n",
    "    spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns\n",
    "])\n",
    "missing_counts.show()\n",
    "\n",
    "\n",
    "# This will display all rows that have at least one null value\n",
    "df.filter(\n",
    "    reduce(lambda a, b: a | b, (col(c).isNull() for c in df.columns))\n",
    ").show()\n",
    "\n",
    "\n",
    "# This gives you the number of rows with at least one null:\n",
    "num_missing_rows = df.filter(\n",
    "    reduce(lambda a, b: a | b, (col(c).isNull() for c in df.columns))\n",
    ").count()\n",
    "print(\"Rows with any missing value:\", num_missing_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "624e12cd-af9f-4d39-9c21-d7fef9c7f54f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "How to deal with missing values if any\n",
    "-drop the missing values ( only if proportion is very less)\n",
    "-fill th emissing values\n",
    "-froward fill\n",
    "-back fill\n",
    "-linear regression\n",
    "-mean values (but it is sensitive to outliers)\n",
    "-median values ( not sensitive to outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1893490-ed17-4293-a85c-4f6937d1c12c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (20000, 21)\n"
     ]
    }
   ],
   "source": [
    "num_rows = df.count()\n",
    "num_cols = len(df.columns)\n",
    "print(f\"Shape: ({num_rows}, {num_cols})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa239c94-82d1-42ee-a633-01730ef8b896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DEALING WITH UNIQUES VALUES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9745e673-ae32-4250-a2e1-8643ac0c68c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------+\n|column_name                 |unique_count|\n+----------------------------+------------+\n|transaction_id              |20000       |\n|user_id                     |2000        |\n|merchant_id                 |1500        |\n|device_id                   |1000        |\n|amount                      |19079       |\n|hour                        |24          |\n|day_of_week                 |7           |\n|transaction_amount_deviation|12603       |\n|is_high_value               |2           |\n|is_odd_hour                 |2           |\n|upi_channel                 |4           |\n|transaction_status          |3           |\n|merchant_category           |6           |\n|device_os                   |4           |\n|default_city                |191         |\n|default_state               |26          |\n|transaction_city            |191         |\n|transaction_state           |26          |\n|fraud                       |2           |\n|transaction_date            |547         |\n+----------------------------+------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "import pandas as pd\n",
    "\n",
    "# Compute unique counts for each column\n",
    "unique_counts = [(c, df.select(countDistinct(c)).first()[0]) for c in df.columns]\n",
    "\n",
    "# Convert to Spark DataFrame for display\n",
    "unique_counts_df = spark.createDataFrame(unique_counts, [\"column_name\", \"unique_count\"])\n",
    "unique_counts_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18ecc51d-ddca-4cf2-a9de-60d5e27971a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Drop the unique values from the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8292ce47-c1d6-457f-a6ca-4d3ef056ac45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+-----------+---------+-------+----+-----------+----------------------------+-------------+-----------+-----------+------------------+-----------------+---------+------------+--------------+----------------+-----------------+-----+----------------+----------------+\n|transaction_id|user_id|merchant_id|device_id|amount |hour|day_of_week|transaction_amount_deviation|is_high_value|is_odd_hour|upi_channel|transaction_status|merchant_category|device_os|default_city|default_state |transaction_city|transaction_state|fraud|transaction_date|transaction_time|\n+--------------+-------+-----------+---------+-------+----+-----------+----------------------------+-------------+-----------+-----------+------------------+-----------------+---------+------------+--------------+----------------+-----------------+-----+----------------+----------------+\n|1             |1614   |889        |389      |293.01 |18  |Saturday   |-78.0                       |0            |0          |PhonePe    |Completed         |Food             |Android  |Jaipur      |Rajasthan     |Ichalkaranji    |Maharashtra      |0    |2023-10-14      |06:40:37 PM     |\n|2             |675    |1312       |855      |1606.29|18  |Friday     |-45.27                      |0            |0          |PhonePe    |Pending           |Utilities        |iOS      |Ujjain      |Madhya Pradesh|Loni            |Uttar Pradesh    |0    |2023-03-03      |06:46:35 PM     |\n|3             |776    |216        |802      |647.77 |0   |Monday     |64.27                       |0            |1          |BHIM       |Failed            |Utilities        |Android  |Rajahmundry |Andhra Pradesh|Raipur          |Chhattisgarh     |0    |2023-07-17      |12:43:03 AM     |\n|4             |715    |1435       |586      |1798.31|21  |Thursday   |-65.85                      |0            |0          |PayTM      |Pending           |Utilities        |Windows  |Mysore      |Karnataka     |Kulti           |West Bengal      |0    |2024-04-18      |09:59:19 PM     |\n|5             |371    |707        |826      |1706.78|2   |Sunday     |-4.63                       |0            |1          |GPay       |Failed            |Utilities        |Android  |Alwar       |Rajasthan     |Chandrapur      |Maharashtra      |1    |2023-12-10      |02:22:35 AM     |\n+--------------+-------+-----------+---------+-------+----+-----------+----------------------------+-------------+-----------+-----------+------------------+-----------------+---------+------------+--------------+----------------+-----------------+-----+----------------+----------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae9db48c-7a6b-4a11-9f78-2d29025ae066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Analysing the classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6e673f0-1870-4ee6-a42c-5e4ac8d0ba97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud cases: 2064\nLegitimate cases: 17936\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# For fraud transactions (fraud = 1)\n",
    "fraud_df = df.filter(col('fraud') == 1)\n",
    "\n",
    "# For legitimate transactions (fraud = 0)\n",
    "normal_df = df.filter(col('fraud') == 0)\n",
    "\n",
    "print(f\"Fraud cases: {fraud_df.count()}\")\n",
    "print(f\"Legitimate cases: {normal_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c55cb2fe-3aab-4ea2-ade1-17bf6f0052db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Machine Learning Models:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c23d534-5a46-47c5-beea-7ed2703f1983",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Label Encoding & Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83090629-f706-44b8-9648-325d73cda751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Label Encoding\n",
    "- Label encoding is a technique used to convert categorical text data into numerical data. Each unique category value is assigned an integer value. This is useful for machine learning algorithms that require numerical input. However, it can introduce ordinal relationships between categories that may not exist.\n",
    "\n",
    "##### Scaling\n",
    "- Scaling is the process of transforming data to fit within a specific range, typically 0 to 1 or -1 to 1. This is important for algorithms that are sensitive to the scale of data, such as gradient descent-based algorithms. Common scaling techniques include Min-Max Scaling and Standardization (Z-score normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90f6ed96-ea0d-4cdc-a466-492c9708058f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   day_of_week_enc  upi_channel_enc  ...  is_odd_hour  fraud\n0                2                3  ...    -0.643527      0\n1                0                3  ...     1.553936      0\n2                1                3  ...    -0.643527      0\n3                1                1  ...    -0.643527      0\n4                2                3  ...    -0.643527      0\n\n[5 rows x 13 columns]\n   day_of_week_enc  upi_channel_enc  ...  is_odd_hour  fraud\n0                2                3  ...    -0.643527      0\n1                0                3  ...     1.553936      0\n2                1                3  ...    -0.643527      0\n3                1                1  ...    -0.643527      0\n4                2                3  ...    -0.643527      0\n\n[5 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Assume pdf is your pandas DataFrame\n",
    "\n",
    "# Categorical columns to encode\n",
    "categorical_cols = [\n",
    "    # 'user_id',\n",
    "    # 'merchant_id',\n",
    "    # 'device_id',\n",
    "    'day_of_week',\n",
    "    'upi_channel',\n",
    "    'transaction_status',\n",
    "    'merchant_category',\n",
    "    'device_os',\n",
    "    # 'default_city',\n",
    "    # 'default_state',\n",
    "    'transaction_city',\n",
    "    'transaction_state'\n",
    "]\n",
    "\n",
    "# Numerical columns to scale\n",
    "numeric_cols = [\n",
    "    'amount',\n",
    "    'hour',\n",
    "    'transaction_amount_deviation',\n",
    "    'is_high_value',\n",
    "    'is_odd_hour'\n",
    "]\n",
    "\n",
    "# 2. Label Encoding for Categorical Columns\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    pdf[col + '_enc'] = le.fit_transform(pdf[col].astype(str))\n",
    "\n",
    "\n",
    "# 3. Standard Scaling for Numerical Columns\n",
    "scaler = StandardScaler()\n",
    "pdf[numeric_cols] = scaler.fit_transform(pdf[numeric_cols])\n",
    "\n",
    "# 4. Prepare final feature list for modeling\n",
    "feature_cols = [col + '_enc' for col in categorical_cols] + numeric_cols\n",
    "\n",
    "# 5. X and y for modeling\n",
    "X = pdf[feature_cols]\n",
    "y = pdf['fraud']\n",
    "\n",
    "# (Optional) Show the first few rows of the processed DataFrame\n",
    "print(pdf[feature_cols + ['fraud']].head())\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import pickle\n",
    "\n",
    "# Categorical and numeric columns\n",
    "categorical_cols = [\n",
    "    'day_of_week',\n",
    "    'upi_channel',\n",
    "    'transaction_status',\n",
    "    'merchant_category',\n",
    "    'device_os',\n",
    "    'transaction_city',\n",
    "    'transaction_state'\n",
    "]\n",
    "\n",
    "numeric_cols = [\n",
    "    'amount',\n",
    "    'hour',\n",
    "    'transaction_amount_deviation',\n",
    "    'is_high_value',\n",
    "    'is_odd_hour'\n",
    "]\n",
    "\n",
    "# 1. Dictionary to store encoders\n",
    "label_encoders = {}\n",
    "\n",
    "# 2. Fit and apply label encoding, store encoders\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    pdf[col + '_enc'] = le.fit_transform(pdf[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# 3. Fit and apply standard scaling\n",
    "scaler = StandardScaler()\n",
    "pdf[numeric_cols] = scaler.fit_transform(pdf[numeric_cols])\n",
    "\n",
    "# 4. Final features and labels\n",
    "feature_cols = [col + '_enc' for col in categorical_cols] + numeric_cols\n",
    "X = pdf[feature_cols]\n",
    "y = pdf['fraud']\n",
    "\n",
    "# Optional: Display few rows\n",
    "print(pdf[feature_cols + ['fraud']].head())\n",
    "\n",
    "# 5. Save encoder dictionary and scaler\n",
    "with open(\"label_encoders.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "with open(\"scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b394228-aabf-4738-8573-76c3a1ba7516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7192ff2-c238-4c0e-ba62-b9ffc59a12a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Train-test split is a technique used to evaluate the performance of a machine learning model. It involves dividing the dataset into two subsets:\n",
    "\n",
    "- Training Set: Used to train the model.\n",
    "- Test Set: Used to evaluate the model's performance on unseen data.\n",
    "\n",
    "This helps in assessing how well the model generalizes to new data. A common practice is to allocate 70-80% of the data for training and the remaining 20-30% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66633683-411e-4e2f-9591-ba25acf5c22c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (14000, 12) (14000,)\nTest shape: (6000, 12) (6000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X = your features DataFrame\n",
    "# y = your target variable (e.g., pdf['fraud'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,         # 30% for testing, 70% for training\n",
    "    random_state=42,       # for reproducibility\n",
    "    stratify=y             # keeps the fraud ratio similar in both sets\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test shape:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67817d9f-3051-49fe-8a4a-db7ab12b2ee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef08c300-cd55-4666-8495-def4b7080bb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "883726c2-d685-4edb-9033-d08845611028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.99      0.98      0.98      5381\n           1       0.83      0.91      0.87       619\n\n    accuracy                           0.97      6000\n   macro avg       0.91      0.94      0.93      6000\nweighted avg       0.97      0.97      0.97      6000\n\nROC AUC Score: 0.9925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# 1. Initialize the Random Forest model\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=100,        # Number of trees\n",
    "    random_state=42,         # For reproducibility\n",
    "    class_weight='balanced'  # Helps with class imbalance\n",
    ")\n",
    "\n",
    "# 2. Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 3. Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]  # Probability of class 1 (fraud)\n",
    "\n",
    "# 4. Evaluate the model\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_proba):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8af67667-25b6-4663-bf38-3054e8c92dce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Summary:\n",
    "The model shows excellent performance, with 97% overall accuracy and a high ROC AUC of 0.9925. It detects 89% of fraud cases (recall) with 84% precision, while non-fraud detection is near perfect. The model is highly effective at distinguishing fraudulent from non-fraudulent transactions, with only a small number of false positives for fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87ffed5e-071b-4409-bdd6-c5f033ba175d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as randomforest.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained model to a file\n",
    "with open('randomforest.pkl', 'wb') as file:\n",
    "    pickle.dump(clf, file)\n",
    "\n",
    "print(\"Model saved as randomforest.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d9e9be8-0865-478c-8350-df1d86b34221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "748506e6-dc9b-4bf5-89f3-5905d2ea3aca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98      5381\n           1       0.80      0.94      0.86       619\n\n    accuracy                           0.97      6000\n   macro avg       0.90      0.95      0.92      6000\nweighted avg       0.97      0.97      0.97      6000\n\nROC AUC Score: 0.9913607352381787\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "\n",
    "# Set scale_pos_weight to balance classes (optional, recommended for imbalanced data)\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "# Initialize and train XGBoost\n",
    "\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    eval_metric='auc',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_proba))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a98ce3b4-62f4-4a35-a06e-390869275c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Summary:\n",
    "The model achieves 97% accuracy and a ROC AUC of 0.99, indicating excellent discrimination between fraud and non-fraud. It detects 94% of fraud cases (recall) with 82% precision. Non-fraud detection remains near perfect. Overall, the model is highly effective for fraud detection, with strong recall for fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2992ddf-b015-4d03-867e-fe5f7e85584b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as xgb_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained model to a file\n",
    "with open('xgb_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "print(\"Model saved as xgb_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5296bf59-545d-439a-a07b-8b55f492da6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7283572781733972,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Fraud Detection in Upi Payments",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}